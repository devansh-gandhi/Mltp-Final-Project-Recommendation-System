{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter products with atleast 15 reviews. USe the processed csv dumped earlier rather than reading the whole data again.\n",
    "df = pd.read_csv('office.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588734, 90502, 10007)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0], df.user_id.nunique(), df.asin.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 90K users, 10k products with 0.58M ratings. We will filter our metadata to these 10k asins only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "asins_of_interest = set(df.asin.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the product description text as a basis for establishing product profiles. No user/rating data is used at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(r\"F:\\work\\is590ml_final\\data\\meta_Office_Products.json.gz\", 'rt', encoding='utf-8') as f:\n",
    "    corpus = {}\n",
    "    n_empty = 0\n",
    "    for line in f:\n",
    "        prod = json.loads(line)\n",
    "        desc = ' '.join(prod.get('description', '')).strip()\n",
    "        if desc:\n",
    "            if prod['asin'] in asins_of_interest:\n",
    "                corpus[prod['asin']] = desc\n",
    "        else:\n",
    "            n_empty += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell loads all descriptions into corpus dictionary keyed by the asin. We note that some products do not have a description. For the rest of the analysis these products are ignored for recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9179574297991406"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)/len(asins_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 9% of products do not have a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_file():\n",
    "    with gzip.open(r\"F:\\work\\is590ml_final\\data\\meta_Office_Products.json.gz\", 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(r\"F:\\work\\is590ml_final\\data\\meta_Office_Products.json.gz\", 'rt', encoding='utf-8') as f:\n",
    "    titles = {}\n",
    "    also_buy = {}\n",
    "    n_empty = 0\n",
    "    for line in f:\n",
    "        prod = json.loads(line)\n",
    "        desc = ' '.join(prod.get('description', '')).strip()\n",
    "        if desc:\n",
    "            if prod['asin'] in asins_of_interest:\n",
    "                titles[prod['asin']] = prod['title']\n",
    "                also_buy[prod['asin']] = prod.get('also_buy', [])\n",
    "        else:\n",
    "            n_empty += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('review_corpus.pickle', 'wb') as f:\n",
    "#     pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus):\n",
    "    \"\"\"Helper function for postprocessing product description and tagged with asins.\"\"\"\n",
    "    for asin, desc in corpus.items():\n",
    "        tokens = gensim.utils.simple_preprocess(desc)\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, [asin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['this', 'rugged', 'covers', 'is', 'ideal', 'for', 'young', 'explorers', 'ages', 'the', 'rubber', 'adventure', 'bible', 'logo', 'durable', 'nylon', 'construction', 'and', 'multiple', 'pockets', 'will', 'encourage', 'kids', 'to', 'take', 'their', 'niv', 'adventure', 'bible', 'with', 'them', 'wherever', 'they', 'go'], tags=['0310802636']),\n",
       " TaggedDocument(words=['featuring', 'metal', 'accents', 'purse', 'style', 'handles', 'and', 'removable', 'key', 'chain', 'this', 'is', 'practical', 'and', 'fashionable', 'bible', 'cover', 'this', 'cover', 'will', 'fit', 'the', 'zondervan', 'niv', 'study', 'bible', 'large', 'print', 'niv', 'life', 'application', 'study', 'bible', 'archaeological', 'study', 'bible', 'as', 'well', 'as', 'many', 'other', 'books', 'and', 'bibles', 'up', 'to', 'mm', 'mm'], tags=['0310821800'])]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create an embedding for each product by considering each product description as a document. The traditional way of doing this is using TF-IDF or LSI. However, since we are dealing with products, I have attempted to use Doc2Vec here (an offshoot of Word2Vec). The advantage of using Doc2Vec is we get an embedding of the whole document (unlike Word2Vec) at once with the nice property that documents pertaining to the same topics have embeddings that are close to each other (parallel to Word2Vec). This way, product profiles for closeby products will be close to each other. As a first pass, I choose 50 dimensions for the embedding and ignore words which do not appear at least twice in the corpus.\n",
    "\n",
    "Since Doc2Vec is based on Word2Vec, it is actually important that stopwords are not removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the doc2vec model. Should not take long if BLAS is installed. We have around 9K documents with a around 50 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check. I now have a model that can maps a product to an embedding. It should stand that a document (product) embedding should actually be closest to itself rather than other documents (product). However, given the model building mechanism of doc2vec, this might not be the case always. As a sanity check, I check how often this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "first_ranks = []\n",
    "for doc in tqdm(train_corpus):\n",
    "    inferred_vector = model.infer_vector(doc.words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [asin for asin, sim in sims].index(doc.tags[0])\n",
    "    ranks.append(rank)\n",
    "    first_ranks.append([doc.tags[0], *sims[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0310802636', '0310802636', 0.9360512495040894],\n",
       " ['0310821800', '0310821800', 0.921605110168457],\n",
       " ['0439893577', '0439893577', 0.9230896830558777],\n",
       " ['0486256006', 'B01DN8T948', 0.9315845966339111],\n",
       " ['0528959948', '0528959948', 0.9450106620788574],\n",
       " ['0545114780', '0545114780', 0.9184267520904541],\n",
       " ['0545114829', '0545114829', 0.9529480934143066],\n",
       " ['0545115000', '0545115000', 0.9598956108093262],\n",
       " ['0545114985', '0545114985', 0.9652101993560791],\n",
       " ['0641678584', '0641678584', 0.9008191823959351]]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_ranks[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15588939690833878"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for i in first_ranks if i[0] != i[1])/len(first_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding model is 85% succesful in distinguishing documents. Frankly, this is way better than I expected given how sparse the description is for many products. Also, I might need to do some CV to figure out the ideal embedding space dimension and training epochs. For now, we collect the product profiles or their 50dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_profiles = {}\n",
    "for doc in train_corpus:\n",
    "    product_profiles[doc.tags[0]] = model.infer_vector(doc.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the product profiles. Now we need to represent individual user preferences or the user profile. Since we do not have any background data on the user, we will model the user based on the ratings given.\n",
    "\n",
    "A rating greater than 3 for product implies that the user has liked the product. So our user profile will be oriented towards the product\n",
    "A rating less than 2 implies the user dislikes the product. So our user profile will be oriented away from the product.\n",
    "A rating of 3 is no particular preference and does not influence the user profile.\n",
    "\n",
    "With these assumptions, I can model the user preferences in the same vector space as the product embedding. User preferences are the weighted sum of their purchased product profiles with centered ratings as the weights. No normalization is done as cosine similarity is going to be used to align products with user preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center the ratings to use as weight\n",
    "df.loc[:, 'rating_weight'] = df.rating - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each user, asin and rating tuple and update user profiles as you go. \n",
    "# If a product does not have a description, it does not get a product profile and does not contribute\n",
    "# to user profiles\n",
    "\n",
    "user_profiles = {}\n",
    "for row in df.itertuples():\n",
    "    user_profiles[row.user_id] = user_profiles.get(row.user_id, np.zeros(50)) + product_profiles.get(row.asin, np.zeros(50)) * row.rating_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.08724699,   2.93460173,   1.71699869,  -9.46520349,\n",
       "        -0.63525108,   8.20420562,  18.98161112,  10.81906489,\n",
       "         1.92793886,   9.27051598,  -9.65438822,   8.67588541,\n",
       "        -6.70450363,  -0.11881249,   0.92907609,  -3.24585358,\n",
       "        -2.32580936,   3.32132585,  -3.2365552 ,  -0.77785248,\n",
       "         8.38153526,  -0.69689466,   7.11275415,   3.24274399,\n",
       "        -1.27049851,   6.34725926,  13.54728842,   2.56984111,\n",
       "        -4.36694187,   8.99892992,   6.84929928, -14.89513856,\n",
       "         6.92706026, -10.42026725,  -0.99373901,  -2.78128792,\n",
       "        -3.20345693,  -7.76043923,  17.36399991,  -6.84747966,\n",
       "         6.0444157 ,  -0.40112205,  -7.14231681,   8.26383809,\n",
       "         6.61044567,  -4.13041114, -11.27173559,  -4.66862772,\n",
       "        -7.16969907,   1.34080695])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_profiles['A398INYG0ZBUZB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user under test\n",
    "user_id = 'A1NK4TLIMODCTN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B00006IFEU Sharpie Permanent Markers, Fine Point, Purple, 12-Pack (30008) 5.0\n",
      "B002CKHH8O Uni Jetstream Multi Function 4 Color Ballpoint Pen, White Barrel (MSXE510007.1) 5.0\n",
      "B00F9ZQ0HI Brother HL-2280DW Wireless Monochrome Multifunction Laser Printer 5.0\n",
      "B00G4CJ8GK Sharpie 1884739 Permanent Markers Fine Point Black - 36 Pieces 5.0\n",
      "B00L95MIJQ 8 PCS Jinhao 599 Fountain Pens Diversity Set Transparent and Unique Style 3.0\n",
      "B015EXS130 Pilot MR Retro Pop Collection Fountain Pen, Red Barrel with Wave Accent, Fine Nib, Black Ink (91432) 5.0\n",
      "B01CWM4E5A Kaisercraft CL101 KaiserColour Gel Pens (24 Pack), 12 Pastel & 12 Glitter Colors, Assorted 5.0\n"
     ]
    }
   ],
   "source": [
    "# print users given ratings when titles exists\n",
    "for row in df.loc[df.user_id == user_id].itertuples():\n",
    "    try:\n",
    "        print(row.asin, titles[row.asin], row.rating)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B001XM9BV8 Brother HL-5370DW Laser Printer 0.8484295606613159\n",
      "B00AP6T05K Brother MFC7460DN Ethernet Monochrome Printer with Scanner, Copier & Fax 0.8423100709915161\n",
      "B00F9ZQ0HI Brother HL-2280DW Wireless Monochrome Multifunction Laser Printer 0.8359221816062927\n",
      "B001XUQP9G Brother HL-5340D High Speed Laser Printer with Duplex 0.8325595855712891\n",
      "B00450DVDY Brother HL-2270DW Compact Laser Printer with Wireless Networking and Duplex 0.827316164970398\n",
      "B016AT5WES Brother Wireless Digital Color Printer with Convenience Copying and Scanning (HL-3180CDW), Amazon Dash Replenishment Enabled 0.8210573792457581\n",
      "B00MFG58N6 Brother MFCL2700DW All-In One Laser Printer with Wireless Networking and Duplex Printing, Amazon Dash Replenishment Enabled 0.818708062171936\n",
      "B0038ZRAES NEW YORK TONER COMPATIBLE WITH BROTHER TN-460 TONER CARTRIDGE (6,000 PAGE YIELD) FOR HL 1435, HL 1440, HL 1450, HL 1470N - BLACK 0.816688597202301\n",
      "B00LEA5EJM Brother HL-L2360DW Compact Laser Printer with Wireless Networking and Duplex, Amazon Dash Replenishment Enabled 0.814453125\n",
      "B00092RJX0 Pilot G2 Retractable Premium Gel Ink Roller Ball Pens, Fine Point, Assorted Colors, 8-Pack (31128) 0.812394380569458\n",
      "B00450DULW Brother Monochrome Laser Printer HL2240 0.8118433952331543\n",
      "B001NVXHB4 Uni-Ball Pipe Lock Drafting 0.5mm Pencil, Black Body with Smoke Accent (M51010.24) 0.8056217432022095\n",
      "B00439GOL8 Brother HL4150CDN Color Laser Printer with Duplex and Networking 0.8051276803016663\n",
      "B013C0ZCE8 Canon MG7720 Wireless All-In-One Printer with Scanner and Copier: Mobile and Tablet Printing, with Airprint  and Google Cloud Print compatible, Red 0.8044297695159912\n",
      "B00MWDUXZ0 Canon imageCLASS LBP6230dw Wireless Laser Printer 0.8036444187164307\n",
      "B0018RHUGG Zebra Clip-On Multifunctional Pen, Non-Color Clear Barrel (B4SA1-C) 0.7979845404624939\n",
      "B000THRJ4Y Pentel Fine Writing Instrument Mechanical Pencil (PG503-ED) 0.7967658042907715\n",
      "B00439GOKY Brother HL4570CDW Color Laser Printer with Wireless Networking and Duplex 0.7964708805084229\n",
      "B002U0H84Y Uni Mechanical Pencil Lead, Nano Dia, 0.5mm, 4B (U05202ND4B) 0.7963482737541199\n",
      "B0016Y7ODK Pilot Gel Ballpoint Pen Hi-Tec-C Coleto 0.4 Refill 10 Color Set (LHKRF1SC410C) 0.7946222424507141\n",
      "B00ZZMRZY2 Towallmark(TM) 10pcs Rainbow Color Pencil 4 in 1 Colored Pencils For Drawing Stationery 0.7944544553756714\n",
      "B00IQV4W2W MUJI Gel Ink Ballpoint Pens 0.38mm 9-colors Pack 0.7931575775146484\n",
      "B01D8Z1VCE Premium 48 Colored Pencils Set By Goodyism – Oil-Based Soft Core – Portable Roll-Up Canvas Bag – Ideal For Adults, Artists, Sketchers & Children – Bonus Mandala E-book & Extra Accessories Included 0.7931309938430786\n",
      "B000WR4NM2 Noodlers Ink 3Oz Aircorp Blue Black 0.7926920056343079\n"
     ]
    }
   ],
   "source": [
    "# print the top 25 recommendations from our model.\n",
    "\n",
    "# user profile\n",
    "u = user_profiles['A1NK4TLIMODCTN']\n",
    "\n",
    "for sim in model.docvecs.most_similar([u], topn=25):\n",
    "    \n",
    "    # some titles are not clean but rather html. To avoid clusttering the output suppress them using a simple len check.\n",
    "    if len(titles[sim[0]]) > 250:\n",
    "        continue\n",
    "    \n",
    "    print(sim[0], titles[sim[0]], sim[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has been more succesful than I expected it to be. The user preferred a brother wireless printer, and our recommender has succesfully pointed out related printers (even trying to upsell higher end models). More impressively, it has recommended toner as well. Similarly, I see a lot of stationary recommendations based on the user purchases. Especially impressive is the the Noodler's ink recommendation since the user has only bought one fountain pen.\n",
    "\n",
    "This is impressive for a content based recommender, because all the product semantics were derived from the description only. One can certainly see how this avoids the cold start problem. If the description is detailed enough, this recommender can certainly pick it up. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
