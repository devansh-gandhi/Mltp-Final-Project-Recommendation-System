{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pickle\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter products with atleast 15 reviews. USe the processed csv dumped earlier rather than reading the whole data again.\n",
    "df = pd.read_csv('office.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0], df.user_id.nunique(), df.asin.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 90K users, 10k products with 0.58M ratings. We will filter our metadata to these 10k asins only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asins_of_interest = set(df.asin.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the product description text as a basis for establishing product profiles. No user/rating data is used at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(r\"F:\\work\\is590ml_final\\data\\meta_Office_Products.json.gz\", 'rt', encoding='utf-8') as f:\n",
    "    corpus = {}\n",
    "    n_empty = 0\n",
    "    for line in f:\n",
    "        prod = json.loads(line)\n",
    "        desc = ' '.join(prod.get('description', '')).strip()\n",
    "        if desc:\n",
    "            if prod['asin'] in asins_of_interest:\n",
    "                corpus[prod['asin']] = desc\n",
    "        else:\n",
    "            n_empty += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell loads all descriptions into corpus dictionary keyed by the asin. We note that some products do not have a description. For the rest of the analysis these products are ignored for recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)/len(asins_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 9% of products do not have a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_file():\n",
    "    with gzip.open(r\"F:\\work\\is590ml_final\\data\\meta_Office_Products.json.gz\", 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(r\"F:\\work\\is590ml_final\\data\\meta_Office_Products.json.gz\", 'rt', encoding='utf-8') as f:\n",
    "    titles = {}\n",
    "    also_buy = {}\n",
    "    n_empty = 0\n",
    "    for line in f:\n",
    "        prod = json.loads(line)\n",
    "        desc = ' '.join(prod.get('description', '')).strip()\n",
    "        if desc:\n",
    "            if prod['asin'] in asins_of_interest:\n",
    "                titles[prod['asin']] = prod['title']\n",
    "                also_buy[prod['asin']] = prod.get('also_buy', [])\n",
    "        else:\n",
    "            n_empty += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('review_corpus.pickle', 'wb') as f:\n",
    "#     pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus):\n",
    "    \"\"\"Helper function for postprocessing product description and tagged with asins.\"\"\"\n",
    "    for asin, desc in corpus.items():\n",
    "        tokens = gensim.utils.simple_preprocess(desc)\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, [asin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create an embedding for each product by considering each product description as a document. The traditional way of doing this is using TF-IDF or LSI. However, since we are dealing with products, I have attempted to use Doc2Vec here (an offshoot of Word2Vec). The advantage of using Doc2Vec is we get an embedding of the whole document (unlike Word2Vec) at once with the nice property that documents pertaining to the same topics have embeddings that are close to each other (parallel to Word2Vec). This way, product profiles for closeby products will be close to each other. As a first pass, I choose 50 dimensions for the embedding and ignore words which do not appear at least twice in the corpus.\n",
    "\n",
    "Since Doc2Vec is based on Word2Vec, it is actually important that stopwords are not removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=75, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the doc2vec model. Should not take long if BLAS is installed. We have around 9K documents with a around 50 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check. I now have a model that can maps a product to an embedding. It should stand that a document (product) embedding should actually be closest to itself rather than other documents (product). However, given the model building mechanism of doc2vec, this might not be the case always. As a sanity check, I check how often this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "first_ranks = []\n",
    "for doc in tqdm(train_corpus):\n",
    "    inferred_vector = model.infer_vector(doc.words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [asin for asin, sim in sims].index(doc.tags[0])\n",
    "    ranks.append(rank)\n",
    "    first_ranks.append([doc.tags[0], *sims[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ranks[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for i in first_ranks if i[0] != i[1])/len(first_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding model is 85% succesful in distinguishing documents. Frankly, this is way better than I expected given how sparse the description is for many products. Also, I might need to do some CV to figure out the ideal embedding space dimension and training epochs. For now, we collect the product profiles or their 50dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_profiles = {}\n",
    "for doc in train_corpus:\n",
    "    product_profiles[doc.tags[0]] = model.infer_vector(doc.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the product profiles. Now we need to represent individual user preferences or the user profile. Since we do not have any background data on the user, we will model the user based on the ratings given.\n",
    "\n",
    "A rating greater than 3 for product implies that the user has liked the product. So our user profile will be oriented towards the product\n",
    "A rating less than 2 implies the user dislikes the product. So our user profile will be oriented away from the product.\n",
    "A rating of 3 is no particular preference and does not influence the user profile.\n",
    "\n",
    "With these assumptions, I can model the user preferences in the same vector space as the product embedding. User preferences are the weighted sum of their purchased product profiles with centered ratings as the weights. No normalization is done as cosine similarity is going to be used to align products with user preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center the ratings to use as weight\n",
    "df.loc[:, 'rating_weight'] = df.rating - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each user, asin and rating tuple and update user profiles as you go. \n",
    "# If a product does not have a description, it does not get a product profile and does not contribute\n",
    "# to user profiles\n",
    "\n",
    "user_profiles = {}\n",
    "for row in df.itertuples():\n",
    "    user_profiles[row.user_id] = user_profiles.get(row.user_id, np.zeros(50)) + product_profiles.get(row.asin, np.zeros(50)) * row.rating_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles['A398INYG0ZBUZB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user under test\n",
    "user_id = 'A1NK4TLIMODCTN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print users given ratings when titles exists\n",
    "for row in df.loc[df.user_id == user_id].itertuples():\n",
    "    try:\n",
    "        print(row.asin, titles[row.asin], row.rating)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top 25 recommendations from our model.\n",
    "\n",
    "# user profile\n",
    "u = user_profiles['A1NK4TLIMODCTN']\n",
    "\n",
    "for sim in model.docvecs.most_similar([u], topn=25):\n",
    "    \n",
    "    # some titles are not clean but rather html. To avoid clusttering the output suppress them using a simple len check.\n",
    "    if len(titles[sim[0]]) > 250:\n",
    "        continue\n",
    "    \n",
    "    print(sim[0], titles[sim[0]], sim[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has been more succesful than I expected it to be. The user preferred a brother wireless printer, and our recommender has succesfully pointed out related printers (even trying to upsell higher end models). More impressively, it has recommended toner as well. Similarly, I see a lot of stationary recommendations based on the user purchases. Especially impressive is the the Noodler's ink recommendation since the user has only bought one fountain pen.\n",
    "\n",
    "This is impressive for a content based recommender, because all the product semantics were derived from the description only. One can certainly see how this avoids the cold start problem. If the description is detailed enough, this recommender can certainly pick it up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atleast 4 reviews\n",
    "df2 = df.groupby('user_id').filter(lambda x: len(x) > 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We withhold 20% of the ratings for each user as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_5star = (df2\n",
    "            .query('rating > 4')\n",
    "            .groupby('user_id')\n",
    "            .apply(lambda x: x.sample(frac=0.2, random_state=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_5star.reset_index(level=0, drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_5star = df2.loc[df2.index.difference(test_set.index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_5star.loc[:, 'rating_weight'] = train_set_5star.rating - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each user, asin and rating tuple and update user profiles as you go. \n",
    "# If a product does not have a description, it does not get a product profile and does not contribute\n",
    "# to user profiles\n",
    "\n",
    "user_profiles_train = {}\n",
    "for row in train_set_5star.itertuples():\n",
    "    user_profiles_train[row.user_id] = user_profiles_train.get(row.user_id, np.zeros(75)) + product_profiles.get(row.asin, np.zeros(75)) * row.rating_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_profiles_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by giving the top 20 recommendations for each user based on their training user profile which withholds 20% of their 5 star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_purchases = train_set.groupby('user_id').asin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco = {}\n",
    "for u, up in tqdm(user_profiles_train.items()):\n",
    "    # get 1000 recos and drop already purchased items\n",
    "    purchases = user_purchases.loc[u]\n",
    "    recos = [p for i,p in \n",
    "             enumerate(filter(lambda x: x[0] not in purchases, model.docvecs.most_similar([up], topn=220)))\n",
    "             if i < 20]\n",
    "    reco[u] = recos\n",
    "                   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=20):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_scores = {}\n",
    "for u in tqdm(test_set_5star.user_id.unique()):\n",
    "    actual = test_set_5star.loc[(test_set_5star.user_id == u), 'asin'].tolist()\n",
    "    predicted = [prod for prod, score in reco[u]]\n",
    "    ap_scores[u] = apk(actual, predicted, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ap_scores.values())/len(ap_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict ratings for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_rating(user, product):\n",
    "    pp = product_profiles[product]\n",
    "    \n",
    "    prods, ratings = [], []\n",
    "    for row in train_set.loc[train_set.user_id==user].itertuples():\n",
    "        try:\n",
    "            prods.append(product_profiles[row.asin])\n",
    "        except Exception:\n",
    "            continue\n",
    "        ratings.append(row.rating)\n",
    "        \n",
    "    \n",
    "    return ratings[most_similar_to(pp, prods)]\n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_to(vec, vec_list):\n",
    "    similarity = -1\n",
    "    nv = norm(vec)\n",
    "    index = None\n",
    "    for i, vec2 in enumerate(vec_list):\n",
    "        sim = cosine_similarity([vec], [vec2])\n",
    "        if sim > similarity:\n",
    "            similarity = sim\n",
    "            index = i\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ratings = {}\n",
    "for row in tqdm(test_set.itertuples()):\n",
    "    try:\n",
    "        pred_ratings[row.Index] = predicted_rating(row.user_id, row.asin)\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ratings = pd.Series(pred_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.sum(np.square((test_set - pred_ratings))))/test_set.shape[0]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
